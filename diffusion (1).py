# -*- coding: utf-8 -*-
"""diffusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u4gT-wXgnP6EGtWmhVaeFrwRPu--4AQx
"""

!pip install torch torchvision torchaudio
!pip install diffusers transformers accelerate safetensors
!pip install pillow matplotlib

import torch
from diffusers import StableDiffusionXLPipeline
import matplotlib.pyplot as plt
from huggingface_hub import login

# Log in to Hugging Face Hub if needed
# Replace 'YOUR_HF_TOKEN' with your actual Hugging Face token
# You can get a token from https://huggingface.co/settings/tokens
login(token='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') # Replace with your token

# Device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load Stable Diffusion model
pipe = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
)

# Enable memory optimizations
# Offload model components to CPU when not in use
pipe.enable_model_cpu_offload()
# Slice VAE decoding into smaller chunks to reduce peak memory usage
pipe.enable_vae_slicing()

# pipe = pipe.to(device) # No longer needed with enable_model_cpu_offload()

# Prompt
prompt = "A futuristic city at night, cyberpunk style, ultra detailed"

# Generate image
image = pipe(
    prompt,
    num_inference_steps=30,
    guidance_scale=7.5
).images[0]

# Show image
plt.imshow(image)
plt.axis("off")

