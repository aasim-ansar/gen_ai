# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K8_6r3lxoioxBz-aBkXpZcxjnp5gbwmG
"""

# Install required packages
!pip install google-generativeai
!pip install langchain
!pip install langchain-google-genai
!pip install pypdf
!pip install faiss-cpu
!pip install tiktoken
!pip install langchain-community
!pip install sentence-transformers langchain

# Import necessary libraries
import google.generativeai as genai
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from operator import itemgetter

# Configure Google Gemini API
GOOGLE_API_KEY = "AIzaSyBmGS1U-EKBRrhnlCPifoDF24tE7O094I0" # Set the variable to the actual API key
genai.configure(api_key=GOOGLE_API_KEY)

# Initialize the Gemini LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=GOOGLE_API_KEY,
    temperature=0.1,
    convert_system_message_to_human=True
)

def direct_llm_answer(question):
    return llm.invoke(question).content

# Re-initialize the LLM, explicitly passing the GOOGLE_API_KEY
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=GOOGLE_API_KEY, # Explicitly pass the API key
    temperature=0.1,
    convert_system_message_to_human=True
)

# Test the LLM
response = llm.invoke("What is EDA?")
print(response.content)

def rag_answer(question):
    return rag_chain.invoke(question)

# Load and process PDF
pdf_reader = PyPDFLoader("/content/RAGPaper+(1).pdf")
documents = pdf_reader.load()
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Create vector store
db = FAISS.from_documents(documents=chunks, embedding=embeddings)

# Create prompt template
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template("""Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:""")

from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from operator import itemgetter

# Create a new prompt for answering with context
ANSWER_PROMPT = PromptTemplate.from_template("""
    You are a document-grounded assistant.

Rules:
- Answer ONLY using the provided context.
- If the answer is not present, say: "The document does not contain this information."
- Cite the source chunk numbers explicitly.
- Do NOT use prior knowledge.

Context:
{context}

Question:
{question}

Answer (with citations):
"""
)

# Define the chain for question generation
question_generator = CONDENSE_QUESTION_PROMPT | llm | str

# Define the retrieval chain
retrieval_chain = (
    {
        "context": itemgetter("standalone_question") | db.as_retriever(),
        "question": itemgetter("standalone_question"),
    }
    | ANSWER_PROMPT
    | llm
)

# Combine everything into the final conversational retrieval chain
qa = (
    RunnablePassthrough.assign(
        standalone_question=question_generator.with_config(run_name="StandaloneQuestion")
    ) | retrieval_chain
)

"""conversational_rag("What is the main topic of the document?")

conversational_rag("Can you summarize it briefly?")

conversational_rag("Why is this important according to the document?")

### Observation

The system successfully maintains conversational context across turns.
Later questions rely on earlier answers, demonstrating functional chat history
and reasoning grounded in retrieved documents.

# Proper conversational memory
chat_history = []

def conversational_rag(question):
    global chat_history
    response = rag_chain.invoke({
        "question": question,
        "chat_history": chat_history
    })
    chat_history.append(("user", question))
    chat_history.append(("assistant", response))
    return response

# Load ALL PDFs from a folder

from langchain.document_loaders import PyPDFLoader
import os

pdf_folder = "data/pdfs"
documents = []

for file in os.listdir(pdf_folder):
    if file.endswith(".pdf"):
        loader = PyPDFLoader(os.path.join(pdf_folder, file))
        documents.extend(loader.load())

# comparison

questions = [
    "What is the main topic discussed in the document?",
    "Explain the key limitations mentioned in the PDF.",
    "What assumptions does the author make?"
]

for q in questions:
    print("QUESTION:", q)
    print("\n--- Direct LLM ---")
    print(direct_llm_answer(q))
    print("\n--- RAG ---")
    print(rag_answer(q))
    print("\n" + "="*80 + "\n")

### Observations

- The direct LLM produces generic answers and sometimes introduces facts not present in the document.
- The RAG system grounds responses in retrieved document chunks.
- RAG answers are more precise, document-specific, and verifiable.

# test

conversational_rag("Summarize the document")
conversational_rag("Can you explain that in simpler terms?")

# Save & Reload FAISS Index (Production Readiness)

vectorstore.save_local("faiss_index")
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectorstore = FAISS.load_local("faiss_index", embeddings)

## Additional Testing & Edge Cases

test_queries = [
    "What is the conclusion of the document?",
    "List all assumptions made by the author.",
    "Does the document mention future work?",
    "What is NOT discussed in the document?",
    "Explain this to a beginner."
]

for q in test_queries:
    print("Q:", q)
    print("A:", rag_answer(q))
    print("-"*60)


# key observasion

## Introduction

This notebook implements a Retrieval-Augmented Generation (RAG) pipeline using LangChain.
The objective is to enhance factual accuracy by grounding LLM responses in external documents.

## System Architecture

1. PDF Loading using PyPDFLoader  
2. Chunking using RecursiveCharacterTextSplitter  
3. Embeddings via all-MiniLM-L6-v2  
4. Vector storage using FAISS  
5. Generation using Gemini 2.5 Flash

## Design Decisions

- Chunk size = 1000 with overlap = 200 to preserve context
- FAISS chosen for fast similarity search
- Gemini Flash selected for low-latency responses

## Limitations

- Performance depends on chunking quality
- Embedding model limits semantic recall
- Requires well-formatted documents






"""